{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hkjS3W5YsXJ-"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import math\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import cv2\n",
        "import pickle\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "x82n7UzNsnWd"
      },
      "outputs": [],
      "source": [
        "def get_speed_wheel(image):\n",
        "    '''\n",
        "    extract the speed and the wheel from the image, the speed is represented by the number of white pixels in the speedometer and their intensity\n",
        "    the wheel is represented by the number of green pixels\n",
        "    both extracted values are normalized:\n",
        "    speed: (0, 1, 2, ..., 20)\n",
        "    wheel: (-10, -8, -7, -5, -4, -2, -1, 0, 1, 2, 4, 5, 7, 8, 10)\n",
        "    '''\n",
        "    speed_img = image[90:94,12:14]\n",
        "    w1 = np.all(speed_img > [240, 240, 240], axis=-1)\n",
        "    w2 = np.all(speed_img > [210, 210, 210], axis=-1)\n",
        "    w3 = np.all(speed_img > [180, 180, 180], axis=-1)\n",
        "    w4 = np.all(speed_img > [150, 150, 150], axis=-1)\n",
        "    w5 = np.all(speed_img > [120, 120, 120], axis=-1)\n",
        "    w6 = np.all(speed_img > [90, 90, 90], axis=-1)\n",
        "    w7 = np.all(speed_img > [60, 60, 60], axis=-1)\n",
        "    w8 = np.all(speed_img > [30, 30, 30], axis=-1)\n",
        "    speed1 = np.sum(w1)\n",
        "    speed2 = np.sum(w2)\n",
        "    speed3 = np.sum(w3)\n",
        "    speed4 = np.sum(w4)\n",
        "    speed5 = np.sum(w5)\n",
        "    speed6 = np.sum(w6)\n",
        "    speed7 = np.sum(w7)\n",
        "    speed8 = np.sum(w8)\n",
        "    speed = math.ceil((speed1 + speed2 + speed3 + speed4 + speed5 + speed6 + speed7 + speed8) / 2)\n",
        "    if speed > 20:\n",
        "        speed = 20 # (0, 1, 2, ..., 20)\n",
        "\n",
        "    wheel_image = image[86:92,36:60]\n",
        "    R, G, B = wheel_image[:, :, 0], wheel_image[:, :, 1], wheel_image[:, :, 2]\n",
        "    green_mask = (G > 250) & (R == 0) & (B == 0)\n",
        "    left_green = green_mask[:,:12]\n",
        "    right_green = green_mask[:,12:]\n",
        "    left_count = np.sum(left_green)\n",
        "    right_count = np.sum(right_green)\n",
        "    if left_count > 0 and right_count > 0:\n",
        "        print('ERR')\n",
        "    green_pixels = max(left_count, right_count)\n",
        "    wheel = math.ceil(green_pixels / 4) # (-7, -6, ..., 0, ..., 6, 7)\n",
        "    if right_count > 0:\n",
        "        wheel *= -1\n",
        "\n",
        "    return speed, wheel\n",
        "\n",
        "def highlight_track(image):\n",
        "    '''\n",
        "    This function removes red and green pixels from the image, removing curbs and grass and highlighting the track\n",
        "    '''\n",
        "    r, g, b = cv2.split(image)\n",
        "    mask_white_only = (r > 130) & (g > 130) & (b > 130)\n",
        "    mask_white = np.zeros_like(mask_white_only, dtype=bool)\n",
        "    mask_white[mask_white_only] = True\n",
        "    mask_diff_only = (np.abs(r - g) > 10) | (np.abs(r - b) > 10) | (np.abs(g - b) > 10)\n",
        "    mask_diff = np.zeros_like(mask_diff_only, dtype=bool)\n",
        "    mask_diff[mask_diff_only] = True\n",
        "    mask = mask_diff | mask_white\n",
        "    image[mask > 0] = [255, 255, 255]\n",
        "    image[mask == 0] = [0, 0, 0]\n",
        "    return image\n",
        "\n",
        "def is_out_of_track(image):\n",
        "    '''\n",
        "    this function checks if the car is out of the track\n",
        "    '''\n",
        "    zoom_state = image[0:64, 14:78]\n",
        "    h_t = highlight_track(zoom_state)\n",
        "    edges = cv2.Canny(h_t, threshold1=80, threshold2=120)\n",
        "    down_indices = np.where(edges[63, :] > 0)[0]\n",
        "    if len(down_indices) == 0:\n",
        "        return True\n",
        "    if len(down_indices) > 2:\n",
        "        down_indices = np.array(sorted(down_indices, key=lambda elem: abs(elem - 32))[:2]) #takes the 2 nearest to center\n",
        "\n",
        "    if (np.all(down_indices > 37) or np.all(down_indices < 27)) and len(down_indices) == 2:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def discretize_state(state):\n",
        "    '''\n",
        "    this function discretize the state in order to use it as a key in the Q table\n",
        "    '''\n",
        "    speed, wheel = get_speed_wheel(state)\n",
        "    zoom_state = state[0:64, 16:80]\n",
        "    zoom_state_resized = cv2.resize(zoom_state, (32, 32), interpolation=cv2.INTER_LINEAR)\n",
        "    h_t = highlight_track(zoom_state_resized)\n",
        "    edges = cv2.Canny(h_t, threshold1=80, threshold2=120)\n",
        "\n",
        "    edges = edges / 255.0 #normalize the input to 0 and 1 more easy for cnn\n",
        "\n",
        "    image_tensor = torch.tensor(edges, dtype=torch.float32).unsqueeze(0)\n",
        "    numeric_values_tensor = torch.tensor([speed, wheel], dtype=torch.float32).unsqueeze(0)\n",
        "    return image_tensor, numeric_values_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "r7vu03Yms_CG"
      },
      "outputs": [],
      "source": [
        "plt.ion() #activate interaction mode\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uhDBBToBZbSH"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, n_actions):\n",
        "          super(DQN, self).__init__()\n",
        "          #self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "          self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=2)  # Input 32x32, Output 16x14x14\n",
        "          self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)  # Input 16x14x14, Output 32x5x5\n",
        "          self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=2)  # Input 32x5x5, Output 32x2x2\n",
        "\n",
        "          # Modifica del fully connected layer\n",
        "          self.fc1 = nn.Linear(32 * 2 * 2, 64)  # Output dopo le convoluzioni: 128 unità\n",
        "\n",
        "          self.fc2 = nn.Linear(2, 16)  # Percorso per speed e wheel\n",
        "\n",
        "          self.fc3 = nn.Linear(64 + 16, 64)  # Combina l'output della CNN e dei valori numerici\n",
        "          self.fc4 = nn.Linear(64, n_actions)\n",
        "\n",
        "    def forward(self, image, numeric_values):\n",
        "        if image.dim() == 3:  # Se l'immagine ha dimensione [1, altezza, larghezza]\n",
        "          image = image.unsqueeze(0)  # Aggiungi la dimensione batch\n",
        "\n",
        "        x = F.relu(self.conv1(image))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output from the CNN\n",
        "        x = F.relu(self.fc1(x))    # Fully connected layer per le caratteristiche estratte dalle convoluzioni\n",
        "\n",
        "        #x = self.dropout(x)  # Applica dropout al fully connected layer\n",
        "\n",
        "        y = F.relu(self.fc2(numeric_values)) # Percorso per i valori numerici (velocità, sterzo)\n",
        "\n",
        "        combined = torch.cat((x, y), dim=1) # Concatenazione dei due percorsi\n",
        "        combined = F.relu(self.fc3(combined))\n",
        "        output = self.fc4(combined)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8tfW8RY8yvO3"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, action_size, gamma=0.99, epsilon=0.9, epsilon_decay=0.999, lr=1e-4):\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque([], maxlen=10000)\n",
        "        self.gamma = gamma    # how much is important the reward of next step, and next of next, ...\n",
        "        self.epsilon = epsilon   # exploration rate\n",
        "        self.epsilon_min = 0.15\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.learning_rate = lr\n",
        "        self.policy_net = DQN(action_size).to(device) #computes Q values for each action in a different state, updated during training (model)\n",
        "        self.target_net = DQN(action_size).to(device) #computes targets to update weights of policy_net (t_model)\n",
        "        self.update_target_net()\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate) # amsgrad=True) -> use AdamW that include a weight decay\n",
        "\n",
        "    def update_target_net(self):\n",
        "        ''' this function updates the target network with the weights of the policy network\n",
        "        '''\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict()) #(initialization) useful to sync the weights of the 2 networks\n",
        "        self.target_net.eval()\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, truncated, terminated):\n",
        "        ''' save the state, action, reward, next_state, truncated, terminated in the memory '''\n",
        "        self.memory.append((state, action, reward, next_state, truncated, terminated))\n",
        "\n",
        "    def act(self, state):\n",
        "        ''' choose an action based on the state, with epsilon-greedy policy'''\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "\n",
        "        im_state, num_state = discretize_state(state)\n",
        "        im_state = im_state.to(device)\n",
        "        num_state = num_state.to(device)\n",
        "\n",
        "        act_values = self.policy_net(im_state, num_state)\n",
        "        return torch.argmax(act_values).item()\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        ''' train the model extracting a batch of samples from the memory '''\n",
        "        minibatch = random.sample(self.memory, batch_size) # take random samples in mem\n",
        "\n",
        "        for state, action, reward, next_state, truncated, terminated in minibatch:\n",
        "\n",
        "            im_state, num_state = discretize_state(state)\n",
        "            im_state = im_state.to(device)\n",
        "            num_state = num_state.to(device)\n",
        "\n",
        "            if not (truncated or terminated):\n",
        "                im_state_next, num_state_next = discretize_state(next_state)\n",
        "                im_state_next = im_state_next.to(device)\n",
        "                num_state_next = num_state_next.to(device)\n",
        "\n",
        "                target = reward + self.gamma * torch.max(self.target_net(im_state_next, num_state_next)).item() #compute target based on the next_state\n",
        "            else:\n",
        "                target = reward\n",
        "\n",
        "            target_f = self.policy_net(im_state, num_state)\n",
        "            target_f[0][action] = target\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss = torch.nn.functional.mse_loss(self.policy_net(im_state, num_state), target_f)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "    def test_trained_model(self, state):\n",
        "        '''\n",
        "        this function is used to test the trained model, it chooses the best action based on the Q values\n",
        "        '''\n",
        "        self.policy_net.eval()\n",
        "\n",
        "        im_state, num_state = discretize_state(state)\n",
        "\n",
        "        im_state = im_state.to(device)\n",
        "        num_state = num_state.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_values = self.policy_net(im_state, num_state)\n",
        "            action = torch.argmax(q_values).item()\n",
        "        return action\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cnAExs3c8JoZ"
      },
      "outputs": [],
      "source": [
        "def train(agent, env, batch_size, num_episodes=100):\n",
        "\n",
        "    truncated = terminated = False\n",
        "\n",
        "    for e in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        step = 0\n",
        "        score = 0\n",
        "        while True:\n",
        "            if step < 45: #skip the first phase of the episode while the image of the output is zooming in\n",
        "                state, _, _, _, _ = env.step(0)\n",
        "                step += 1\n",
        "                continue\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, truncated, terminated, info = env.step(action)\n",
        "            if is_out_of_track(next_state): #check if the car is out of the track\n",
        "                print('OUT')\n",
        "                reward = -100\n",
        "                terminated = True\n",
        "            agent.remember(state, action, reward, next_state, truncated, terminated)\n",
        "            score += reward\n",
        "            state = next_state\n",
        "            if truncated or terminated:\n",
        "                agent.update_target_net() # update the target network at the end of each episode\n",
        "                print(f\"Episode: {e + 1}/{num_episodes}, epsilon: {agent.epsilon}, Episode-reward: {score}, steps_done: {step}\")\n",
        "                break\n",
        "            step += 1\n",
        "        agent.replay(batch_size) #train the model with a batch of samples from the memory at the end of each episode\n",
        "        if agent.epsilon > agent.epsilon_min:\n",
        "            agent.epsilon *= agent.epsilon_decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "tVH5GRmKgc8J"
      },
      "outputs": [],
      "source": [
        "def evaluate(agent, env, num_episodes=100, render=False, random_a=False):\n",
        "    total_score = 0\n",
        "    for _ in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        truncated = terminated = False\n",
        "        episode_score = 0\n",
        "        step = 0\n",
        "        while not (truncated or terminated):\n",
        "            if step < 45: #skip the first zoom samples\n",
        "                state, _, _, _, _ = env.step(0)\n",
        "                step += 1\n",
        "                continue\n",
        "            if random_a:\n",
        "                action = random.randint(0, 4)\n",
        "            else:\n",
        "                action = agent.test_trained_model(state)\n",
        "            next_state, reward, truncated, terminated, _ = env.step(action)\n",
        "            # if is_out_of_track(next_state):\n",
        "            #     print('OUT')\n",
        "            #     reward = -100\n",
        "            #     terminated = True\n",
        "            episode_score += reward\n",
        "            state = next_state\n",
        "            if render and random.randint(0, 3) == 0:\n",
        "                img = env.render()\n",
        "                img_to_save = Image.fromarray(img)\n",
        "                img_to_save.save(f'./im_good/{step}.png')\n",
        "                plt.imshow(img)\n",
        "                plt.axis('off')\n",
        "                plt.show()\n",
        "                clear_output(wait=True)\n",
        "            step += 1\n",
        "        total_score += episode_score\n",
        "    average_score = total_score / num_episodes\n",
        "    return average_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0NnhaGHm3Cft"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CarRacing-v2\", domain_randomize=False, continuous=False, render_mode=\"rgb_array\")\n",
        "\n",
        "batch_size = 512\n",
        "episodes = 3000\n",
        "gamma = 0.99\n",
        "epsilon = 1\n",
        "epsilon_decay = 0.9992\n",
        "lr = 1e-4\n",
        "\n",
        "agent = DQNAgent(env.action_space.n, gamma=gamma, epsilon=epsilon, epsilon_decay=epsilon_decay, lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SveT6rt13CMs",
        "outputId": "21883b22-e8b2-422e-8520-c14181613e14"
      },
      "outputs": [],
      "source": [
        "print(evaluate(agent, env, 15, render=False, random_a=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjZRJeUF8S-N",
        "outputId": "f4cc6973-3592-49d1-b130-60cfb722fd57"
      },
      "outputs": [],
      "source": [
        "train(agent, env, batch_size, episodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(evaluate(agent, env, 15, render=False, random_a=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "''' save the agent, the environment and the hyperparameters and the results of the evaluation '''\n",
        "# with open('DQN/env7.pkl', 'wb') as f:\n",
        "# \tpickle.dump(env, f)\n",
        "# with open('DQN/agent7.pkl', 'wb') as f:\n",
        "# \tpickle.dump(agent, f)\n",
        "# with open('hyperparameters7.txt', 'w') as f:\n",
        "# \tf.write(f'batch_size: {batch_size}\\n')\n",
        "# \tf.write(f'episodes: {episodes}\\n')\n",
        "# \tf.write(f'gamma: {gamma}\\n')\n",
        "# \tf.write(f'epsilon: {epsilon}\\n')\n",
        "# \tf.write(f'epsilon_decay: {epsilon_decay}\\n')\n",
        "# \tf.write(f'lr: {lr}\\n')\n",
        "# \tf.write(f'result_random_15_ep: {evaluate(agent, env, 15, render=False, random_a=True)}\\n')\n",
        "# \tf.write(f'result_trained_15_ep: {evaluate(agent, env, 15, render=False, random_a=False)}\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "891.5550161812156\n"
          ]
        }
      ],
      "source": [
        "''' load pretrained model with pkl and test it '''\n",
        "# with open('DQN/agent1.pkl', 'rb') as f:\n",
        "# \tagent_test = pickle.load(f)\n",
        "# with open('DQN/env1.pkl', 'rb') as f:\n",
        "# \tenv_test = pickle.load(f)\n",
        "\n",
        "# print(evaluate(agent_test, env_test, 1, render=True))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
